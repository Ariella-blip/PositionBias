{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bd43eb-9294-47d2-89f4-640971a09084",
   "metadata": {},
   "source": [
    "# Algorithms used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad919f46-77eb-4462-be29-61a7074a81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2127271b-4a2c-4ab6-a271-27011780873a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_test_split(data, sim = True):\n",
    "    \"\"\"\n",
    "    Initialize a train and test split based on search sessions\n",
    "    Test set consists of 10% of the total data\n",
    "    Returns all necessary arrays for the EM algorithm\n",
    "    \"\"\"\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.01, random_state=42)\n",
    "    \n",
    "    # Split data indices based on search sessions\n",
    "    if sim:\n",
    "        train_idx, test_idx = next(gss.split(data, groups=data.index))\n",
    "    else:\n",
    "        train_idx, test_idx = next(gss.split(data, groups=data['session']))\n",
    "        \n",
    "    train = data.iloc[train_idx].copy()\n",
    "    test = data.iloc[test_idx].copy()\n",
    "    \n",
    "    train['qd_id'] = train['qd_id'].cat.remove_unused_categories()\n",
    "    test['qd_id'] = test['qd_id'].cat.remove_unused_categories()\n",
    "    \n",
    "    clicks = train['click'].astype(float).to_numpy()\n",
    "    ranks = train['rank'].astype(int).to_numpy()\n",
    "    qd_ids = train['qd_id'].cat.codes.astype(int).to_numpy()\n",
    "    \n",
    "    clicks_test = test['click'].astype(float).to_numpy()\n",
    "    ranks_test = test['rank'].astype(int).to_numpy()\n",
    "    qd_ids_test = test['qd_id'].cat.codes.astype(int).to_numpy()\n",
    "    \n",
    "    return clicks, ranks, qd_ids, clicks_test, ranks_test, qd_ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da4793e8-05a3-4f83-8f36-47cb50c4154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_bias = np.array([0.7, 0.6, 0.5, 0.4, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7122ced-4d57-4494-9d42-5cd5b0aa60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(v1, v2):\n",
    "    \"\"\"\n",
    "    Calculates euclidean distance to the true position bias estimates\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(\n",
    "        (v1 / v1[0] - v2 / v2[0]) ** 2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6ae21b7-2380-48d7-a44c-8fb7658fde49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_likelihood(clicks, E, R):\n",
    "    \"\"\"\n",
    "    Calculates the log likelihood for the EM algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    click_probs = np.where(clicks == 1, E * R, 1 - (E * R))\n",
    "    click_probs[click_probs == 0] = np.finfo(float).eps  # to deal with zeroes\n",
    "    \n",
    "    return np.log(click_probs + 0.001).sum() / len(click_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09631966-321e-43b0-893c-f6e5ddf3f9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def E_step(bias, ranks, relevance, qd_ids, llambda):\n",
    "    \"\"\"\n",
    "    Calculates the Expectation-step for the EM algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    nonp = 1 - (bias[ranks] * relevance[qd_ids])\n",
    "    nonp += llambda\n",
    "    E = (bias[ranks] * (1 - relevance[qd_ids])) / nonp\n",
    "    R = ((1 - bias[ranks]) * relevance[qd_ids]) / nonp\n",
    "    \n",
    "    return E, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b475e745-0a08-4f4f-bb40-13cfe6a15ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def M_step(clicks, ranks, ranks_cnt, qd_ids, qd_cnt, E, R):\n",
    "    \"\"\"\n",
    "    Calculates the Maximization-step for the EM algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    bias = np.bincount(ranks, weights=clicks + (1 - clicks) * E) / ranks_cnt\n",
    "    relevance = np.bincount(qd_ids, weights=clicks + (1 - clicks) * R ) / qd_cnt\n",
    "    \n",
    "    return bias, relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9654de28-ed8b-4398-bb62-b3d1a3e2677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(data, alpha = 0, beta = 0, learning_rate = 0.1, llambda = 0.1, max_iter = 1000, tol = 1e-6):\n",
    "    \"\"\"\n",
    "    Conducts the Expectation-Maximization (EM) Algorithm\n",
    "    When alpha/beta is set to zero it conducts the regular EM\n",
    "    \"\"\"\n",
    "     \n",
    "    # Create train-test split \n",
    "    clicks, ranks, qd_ids, clicks_test, ranks_test, qd_ids_test = train_test_split(data) \n",
    "    \n",
    "    ranks_cnt = np.bincount(ranks) \n",
    "    bias = np.bincount(ranks, weights=clicks) / ranks_cnt\n",
    "    qd_cnt = np.bincount(qd_ids)\n",
    "    relevance = np.bincount(qd_ids, weights=clicks) / qd_cnt\n",
    "    \n",
    "    baseline = bias.copy()\n",
    "    \n",
    "    loglikelihoods = []\n",
    "    loglikelihoods_test = []\n",
    "    distances = []\n",
    "\n",
    "    for itt in range(max_iter):\n",
    "        distances.append(diff(bias, true_bias))\n",
    "        bias_old, relevance_old = bias.copy(), relevance.copy()\n",
    "        \n",
    "        # Expectation step for train and test set\n",
    "        E, R = E_step(bias, ranks, relevance, qd_ids, llambda)      \n",
    "        E_test, R_test = E_step(bias, ranks_test, relevance, qd_ids_test, llambda)\n",
    "        \n",
    "        # Log Likelihood for train and test set\n",
    "        loglikelihoods.append(log_likelihood(clicks, E, R))\n",
    "        loglikelihoods_test.append(log_likelihood(clicks_test, E_test, R_test))        \n",
    "        \n",
    "        # Maximization step based on only train set\n",
    "        bias_new, relevance_new = M_step(clicks, ranks, ranks_cnt, qd_ids, qd_cnt, E, R)\n",
    "        \n",
    "        # Bounds EM by priors, if a = b = 0 then no priors are used\n",
    "        if alpha != 0 & beta != 0:\n",
    "            relevance_new = (alpha +  np.round(qd_cnt * relevance_new)) / (alpha + beta + qd_cnt)\n",
    "        \n",
    "        # Updates parameters with learning rate\n",
    "        bias += (bias_new - bias) * learning_rate\n",
    "        relevance += (relevance_new - relevance) * learning_rate\n",
    "        \n",
    "        # Convergence criteria\n",
    "        if np.max(np.abs(bias - bias_old)) < tol and np.max(np.abs(relevance - relevance_old)) < tol:\n",
    "            break\n",
    "            \n",
    "    return bias, relevance, baseline, distances, loglikelihoods, loglikelihoods_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ba8c5ec-f40c-458c-8fc1-e60d317fc678",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:EM is <function EM at 0x7fcb7a9e72e0>\n",
      "Proper storage of interactively declared classes (or instances\n",
      "of those classes) is not possible! Only instances\n",
      "of classes in real modules on file system can be %store'd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%store EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b95b98f1-8a94-4318-a09f-e9d1814f9570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Dense Data, Rank-Relevance correlation:  ---------------------------------------------------------------  --------------\n",
      "Amount of query-document pairs that occurred only once              0.0849979\n",
      "Amount of query-document pairs that occurred 2-10 times:            0.488738\n",
      "Amount of query-document pairs that occurred more than 10 times    99.4263\n",
      "Highest occurence count:                                         5369\n",
      "Amount of rows:                                                     3.08152e+06\n",
      "Amount of query-document pairs:                                  4706\n",
      "---------------------------------------------------------------  --------------\n",
      "Dense Data, No Rank-Relevance correlation:  ---------------------------------------------------------------  --------------\n",
      "Amount of query-document pairs that occurred only once              1.00032\n",
      "Amount of query-document pairs that occurred 2-10 times:            1.80703\n",
      "Amount of query-document pairs that occurred more than 10 times    97.1926\n",
      "Highest occurence count:                                         2092\n",
      "Amount of rows:                                                     3.14349e+06\n",
      "Amount of query-document pairs:                                  3099\n",
      "---------------------------------------------------------------  --------------\n",
      "Sparse Data, Uniform Relevance:  ---------------------------------------------------------------  -----------\n",
      "Amount of query-document pairs that occurred only once              87.2982\n",
      "Amount of query-document pairs that occurred 2-10 times:             6.94882\n",
      "Amount of query-document pairs that occurred more than 10 times      5.75295\n",
      "Highest occurence count:                                            91\n",
      "Amount of rows:                                                  48360\n",
      "Amount of query-document pairs:                                  20320\n",
      "---------------------------------------------------------------  -----------\n",
      "Sparse Data, Non-Uniform Relevance:  ---------------------------------------------------------------  -----------\n",
      "Amount of query-document pairs that occurred only once              87.67\n",
      "Amount of query-document pairs that occurred 2-10 times:             9.01847\n",
      "Amount of query-document pairs that occurred more than 10 times      3.31154\n",
      "Highest occurence count:                                           214\n",
      "Amount of rows:                                                  38610\n",
      "Amount of query-document pairs:                                  20957\n",
      "---------------------------------------------------------------  -----------\n",
      "Warning:simulate_search_results is <function simulate_search_results at 0x7fcb3266aef0>\n",
      "Proper storage of interactively declared classes (or instances\n",
      "of those classes) is not possible! Only instances\n",
      "of classes in real modules on file system can be %store'd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run Simulations.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f691db0e-03b7-4d97-b942-a0cf2732216c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence Reached\n",
      "Convergence Reached\n"
     ]
    }
   ],
   "source": [
    "# Regular EM on dense data scenarios\n",
    "results_dense_corr = EM(dense_data_corr) # rank-relevance correlation\n",
    "results_dense_random = EM(dense_data_random) # no rank-relevance correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b80bfd02-6b0e-43de-8ead-78e26274cee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Simulation R-R correlation \n",
      " Iterations:  442 \n",
      " Euclidean Distance:  0.09705657750994029 \n",
      " LL train set : -0.5505569159282709 \n",
      " LL test set:  -0.6907556603361568\n",
      "\n",
      " Dense Simulation No R-R correlation \n",
      " Iterations:  379 \n",
      " Euclidean Distance:  0.15127964014740455 \n",
      " LL train set : -0.6021822896001325 \n",
      " LL test set:  -0.6267978198923799\n"
     ]
    }
   ],
   "source": [
    "# Report\n",
    "print(\"Dense Simulation,  R-R correlation \\n\", \"Iterations: \", len(results_dense_corr[4]), \"\\n Euclidean Distance: \", results_dense_corr[3][-1], \"\\n LL train set :\", results_dense_corr[4][-1],  \"\\n LL test set: \", results_dense_corr[5][-1])\n",
    "print(\"\\n Dense Simulation, No R-R correlation \\n\", \"Iterations: \", len(results_dense_random[4]), \"\\n Euclidean Distance: \", results_dense_random[3][-1], \"\\n LL train set :\", results_dense_random[4][-1],  \"\\n LL test set: \", results_dense_random[5][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0d5f3544-16b7-4959-a206-43120c9a9fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sparse Data Scenarios\n",
    "\n",
    "# Runs different priors and regular EM\n",
    "alphas = [0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "betas = [0, 1, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
    "\n",
    "uniform_results = [EM(sparse_data_uniform, alpha = alpha, beta = beta) for alpha, beta in zip(alphas, betas)]\n",
    "peaked_results = [EM(sparse_data_peaked, alpha = alpha, beta = beta) for alpha, beta in zip(alphas, betas)]\n",
    "\n",
    "alphas[0], betas[0] = \"Regular EM\", \"Regular EM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c769a903-aad1-49a1-8421-9b9682df3967",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sparse Simulation, Uniform Relevance \n",
      "\n",
      "+----+------------+------------+-----------------------+-----------------------------+----------------------------+\n",
      "|    | Alpha      | Beta       |   Euclidean Distances |   Log Likelihoods Train Set |   Log Likelihoods Test Set |\n",
      "|----+------------+------------+-----------------------+-----------------------------+----------------------------|\n",
      "|  0 | Regular EM | Regular EM |              0.200077 |                   -0.910362 |                  -1.40578  |\n",
      "|  1 | 1          | 1          |              0.144884 |                   -0.557437 |                  -0.628729 |\n",
      "|  2 | 1          | 9          |              0.252598 |                   -0.725014 |                  -0.899237 |\n",
      "|  3 | 2          | 8          |              0.150735 |                   -0.670582 |                  -0.769979 |\n",
      "|  4 | 3          | 7          |              0.13725  |                   -0.629909 |                  -0.689405 |\n",
      "|  5 | 4          | 6          |              0.195041 |                   -0.600397 |                  -0.633432 |\n",
      "|  6 | 5          | 5          |              0.266652 |                   -0.579915 |                  -0.593432 |\n",
      "|  7 | 6          | 4          |              0.333676 |                   -0.569485 |                  -0.567905 |\n",
      "|  8 | 7          | 3          |              0.40597  |                   -0.572243 |                  -0.556083 |\n",
      "|  9 | 8          | 2          |              0.468021 |                   -0.593614 |                  -0.582631 |\n",
      "| 10 | 9          | 1          |              0.509683 |                   -0.650136 |                  -0.670632 |\n",
      "+----+------------+------------+-----------------------+-----------------------------+----------------------------+\n"
     ]
    }
   ],
   "source": [
    "dist = [l[3][-1] for l in uniform_results]\n",
    "lls = [l[4][-1] for l in uniform_results]\n",
    "lls_test = [l[5][-1] for l in uniform_results]\n",
    "\n",
    "print(\"\\n Sparse Simulation, Uniform Relevance \\n\")\n",
    "df1 = pd.DataFrame({'Alpha': alphas, 'Beta': betas, 'Euclidean Distances': dist, 'Log Likelihoods Train Set': lls, 'Log Likelihoods Test Set': lls_test})\n",
    "\n",
    "print(tabulate(df1, headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fe06bd51-fdb5-4005-ae4d-132104891353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sparse Simulation, Non-Uniform Relevance \n",
      "\n",
      "+----+------------+------------+-----------------------+-----------------------------+----------------------------+\n",
      "|    | Alpha      | Beta       |   Euclidean Distances |   Log Likelihoods Train Set |   Log Likelihoods Test Set |\n",
      "|----+------------+------------+-----------------------+-----------------------------+----------------------------|\n",
      "|  0 | Regular EM | Regular EM |             0.426901  |                   -0.820268 |                  -1.00967  |\n",
      "|  1 | 1          | 1          |             0.0446825 |                   -0.471212 |                  -0.445864 |\n",
      "|  2 | 1          | 9          |             0.372091  |                   -0.590077 |                  -0.611385 |\n",
      "|  3 | 2          | 8          |             0.191578  |                   -0.543125 |                  -0.523697 |\n",
      "|  4 | 3          | 7          |             0.0586518 |                   -0.513512 |                  -0.473874 |\n",
      "|  5 | 4          | 6          |             0.0993581 |                   -0.496593 |                  -0.443169 |\n",
      "|  6 | 5          | 5          |             0.19283   |                   -0.490052 |                  -0.425369 |\n",
      "|  7 | 6          | 4          |             0.269094  |                   -0.493288 |                  -0.418231 |\n",
      "|  8 | 7          | 3          |             0.34165   |                   -0.509447 |                  -0.425505 |\n",
      "|  9 | 8          | 2          |             0.418331  |                   -0.543053 |                  -0.460802 |\n",
      "| 10 | 9          | 1          |             0.399678  |                   -0.609391 |                  -0.535165 |\n",
      "+----+------------+------------+-----------------------+-----------------------------+----------------------------+\n"
     ]
    }
   ],
   "source": [
    "dist = [l[3][-1] for l in peaked_results]\n",
    "lls = [l[4][-1] for l in peaked_results]\n",
    "lls_test = [l[5][-1] for l in peaked_results]\n",
    " \n",
    "print(\"\\n Sparse Simulation, Non-Uniform Relevance \\n\")\n",
    "\n",
    "df2 = pd.DataFrame({'Alpha': alphas, 'Beta': betas, 'Euclidean Distances': dist, 'Log Likelihoods Train Set': lls, 'Log Likelihoods Test Set': lls_test})\n",
    "\n",
    "print(tabulate(df2, headers = 'keys', tablefmt = 'psql'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
